{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrPGXN9VuExre/P52fC9+P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharon-1234/CodeAlpha_Task/blob/main/CodeAlpha_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "iB44RSSwzB-x",
        "outputId": "82940a76-7440-4e60-c434-a1f71dba144f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-6-fd81832a732b>, line 22)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-fd81832a732b>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    ```python\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('credit_data.csv')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(data.head())\n",
        "\n",
        "# Split data into features (X) and target (y)\n",
        "X = data.drop('creditworthy', axis=1)  # Features\n",
        "y = data['creditworthy']  # Target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the feature values\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('credit_data.csv')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(data.head())\n",
        "\n",
        "# Split data into features (X) and target (y)\n",
        "X = data.drop('creditworthy', axis=1)  # Features\n",
        "y = data['creditworthy']  # Target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the feature values\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "```\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize the models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Support Vector Machine': SVC()\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f'{model_name} Accuracy: {accuracy:.2f}')\n",
        "    print(classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Example: Fine-tuning the Random Forest model\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and best score\n",
        "print(f'Best Parameters: {grid_search.best_params_}')\n",
        "print(f'Best Score: {grid_search.best_score_:.2f}')\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(f'Test Set Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n",
        "print(classification_report(y_test, y_pred))\n",
        "```\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize the models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Support Vector Machine': SVC()\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f'{model_name} Accuracy: {accuracy:.2f}')\n",
        "    print(classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Example: Fine-tuning the Random Forest model\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and best score\n",
        "print(f'Best Parameters: {grid_search.best_params_}')\n",
        "print(f'Best Score: {grid_search.best_score_:.2f}')\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(f'Test Set Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n",
        "print(classification_report(y_test, y_pred))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2"
      ],
      "metadata": {
        "id": "-sCGWi810IkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "W0BLkKcZ2zn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Define a function to extract features from an audio file\n",
        "def extract_features(file_path):\n",
        "    # Load the audio file\n",
        "    audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast')\n",
        "\n",
        "    # Extract MFCC features\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "\n",
        "    # Compute the mean of the MFCCs\n",
        "    mfccs_mean = np.mean(mfccs, axis=1)\n",
        "\n",
        "    return mfccs_mean\n",
        "\n",
        "# Path to the dataset\n",
        "dataset_path = 'path_to_ravdess_dataset'\n",
        "\n",
        "# Initialize lists to store features and labels\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "# Map emotions to numerical labels\n",
        "emotion_map = {\n",
        "    'angry': 0,\n",
        "    'happy': 1,\n",
        "    'sad': 2,\n",
        "    'neutral': 3,\n",
        "    # Add other emotions if present in the dataset\n",
        "}\n",
        "\n",
        "# Loop through each file in the dataset and extract features\n",
        "for root, _, files in os.walk(dataset_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.wav'):\n",
        "            file_path = os.path.join(root, file)\n",
        "            # Extract features\n",
        "            mfccs_mean = extract_features(file_path)\n",
        "            features.append(mfccs_mean)\n",
        "\n",
        "            # Extract emotion label from the file name (e.g., '03-01-05-01-01-01-01.wav' where '05' indicates 'angry')\n",
        "            emotion = file.split('-')[2]\n",
        "            labels.append(emotion_map[emotion])\n",
        "\n",
        "# Convert features and labels to numpy arrays\n",
        "X = np.array(features)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Save features and labels to CSV files for later use\n",
        "np.savetxt('features.csv', X, delimiter=',')\n",
        "np.savetxt('labels.csv', y, delimiter=',')\n",
        "```\n",
        "\n",
        "### Step 3: Model Selection and Training\n",
        "We'll use TensorFlow/Keras to build a deep learning model for emotion classification.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "# Load features and labels\n",
        "X = np.loadtxt('features.csv', delimiter=',')\n",
        "y = np.loadtxt('labels.csv', delimiter=',')\n",
        "\n",
        "# One-hot encode the labels\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "y = onehot_encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    Dense(256, input_shape=(X_train.shape[1],), activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(y_train.shape[1], activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f'Test accuracy: {test_acc:.2f}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "02ii9m3y0MTh",
        "outputId": "345eff51-ed49-4593-aecf-c50597c96da6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 58) (<ipython-input-9-3f77ddae8224>, line 58)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-3f77ddae8224>\"\u001b[0;36m, line \u001b[0;32m58\u001b[0m\n\u001b[0;31m    We'll use TensorFlow/Keras to build a deep learning model for emotion classification.\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 58)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3"
      ],
      "metadata": {
        "id": "02slRI0H04i0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the dataset (for simplicity, using MNIST here, but you can replace it with EMNIST)\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "# Reshape the images to add a channel dimension (required for CNN)\n",
        "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\n",
        "\n",
        "# Convert the labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "```\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "model = Sequential\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "```\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Plot training & validation accuracy and loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### Step 5: Model Evaluation\n",
        "Evaluate the model on the test set to see how well it performs.\n",
        "\n",
        "```python\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f'Test accuracy: {test_acc:.2f}')\n",
        "```\n",
        "\n",
        "### Step 6: Model Prediction\n",
        "Use the trained model to make predictions on new handwritten characters.\n",
        "\n",
        "```python\n",
        "# Predict on a new sample (e.g., the first test sample)\n",
        "sample = X_test[0].reshape(1, 28, 28, 1)\n",
        "prediction = model.predict(sample)\n",
        "predicted_class = np.argmax(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "cGP8ZD420_eX",
        "outputId": "fb61c49d-8eaa-4079-c1a4-f26fee73f6a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unmatched ']' (<ipython-input-11-5f7e97f882c2>, line 34)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-5f7e97f882c2>\"\u001b[0;36m, line \u001b[0;32m34\u001b[0m\n\u001b[0;31m    ])\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ']'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4"
      ],
      "metadata": {
        "id": "s8_NHv3820ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('medical_data.csv')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(data.head())\n",
        "\n",
        "# Check for missing values\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Fill missing values or drop rows/columns with missing values\n",
        "data = data.dropna()  # For simplicity, dropping rows with missing values\n",
        "\n",
        "# Split data into features (X) and target (y)\n",
        "X = data.drop('disease', axis=1)  # Features\n",
        "y = data['disease']  # Target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the feature values\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "```\n",
        "\n",
        "### Step 3: Model Selection\n",
        "We'll use a few classification algorithms to find the best model for predicting the disease. Common choices include Logistic Regression, Decision Trees, Random Forests, and Support Vector Machines (SVM).\n",
        "\n",
        "### Step 4: Model Training and Evaluation\n",
        "We'll train and evaluate each model using accuracy and other relevant metrics like precision, recall, and F1-score.\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize the models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Support Vector Machine': SVC()\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f'{model_name} Accuracy: {accuracy:.2f}')\n",
        "    print(classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "### Step 5: Model Fine-Tuning\n",
        "Based on the evaluation, you can fine-tune the best-performing model using techniques like hyperparameter tuning with grid search or random search.\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Example: Fine-tuning the Random Forest model\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and best score\n",
        "print(f'Best Parameters: {grid_search.best_params_}')\n",
        "print(f'Best Score: {grid_search.best_score_:.2f}')\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(f'Test Set Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "wVaM3_Ap22as",
        "outputId": "9b9861a5-1150-4a12-dd8b-bd173de42f29"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 31) (<ipython-input-12-2976d8057534>, line 31)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-2976d8057534>\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    We'll use a few classification algorithms to find the best model for predicting the disease. Common choices include Logistic Regression, Decision Trees, Random Forests, and Support Vector Machines (SVM).\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 31)\n"
          ]
        }
      ]
    }
  ]
}